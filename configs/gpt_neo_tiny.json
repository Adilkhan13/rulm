{
    "block_size": 1024,
    "model": {
        "type": "EleutherAI/gpt-neo-125M",
        "attention_dropout": 0.1,
        "num_layers": 6,
        "num_heads": 8,
        "hidden_size": 512,
        "use_cache": true
    },
    "trainer": {
        "batch_size": 20,
        "gradient_accumulation_steps": 8,
        "eval_steps": 2000,
        "logging_steps": 50,
        "num_train_epochs": 3,
        "learning_rate": 0.0005,
        "max_steps": 10000,
        "lr_scheduler_type": "cosine",
        "warmup_steps": 1000,
        "weight_decay": 0.1,
        "fp16": true,
        "gradient_checkpointing": false
    }
}
