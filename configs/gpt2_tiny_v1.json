{
    "block_size": 512,
    "model": {
        "type": "gpt2",
        "n_ctx": 512,
        "n_positions": 512,
        "n_embd": 256,
        "n_head": 8,
        "n_layer": 6,
        "use_cache": true,
        "attn_pdrop": 0.01,
        "embd_pdrop": 0.01
    },
    "trainer": {
        "batch_size": 56,
        "gradient_accumulation_steps": 6,
        "eval_steps": 1000,
        "logging_steps": 20,
        "learning_rate": 0.001,
        "max_steps": 20000,
        "lr_scheduler_type": "cosine",
        "warmup_steps": 1000,
        "weight_decay": 0.1,
        "fp16": true,
        "gradient_checkpointing": false,
        "optim": "adamw_apex_fused",
        "half_precision_backend": "auto",
        "fp16_opt_level": "O2"
    }
}
